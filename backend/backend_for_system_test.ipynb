{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "# ==============================================================================\n",
    "# backend_for_system_test.ipynb\n",
    "# Take user input, process and attempt to detect probability that the article is fake news for test purpose.\n",
    "# Does not generate final explanation to save time\n",
    "#\n",
    "# Written by: Honggyo Suh <honggyo.suh@student.unsw.edu.au>\n",
    "# Date: 2023-11-02\n",
    "# For TWEETTRUTH fake news detection system\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import openai\n",
    "import textstat\n",
    "import nltk\n",
    "import sys\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "import time\n",
    "\n",
    "\n",
    "# Our system class contains model, scaler, API key, and statistic results from analysis\n",
    "class Backend_system:\n",
    "    def __init__(self):\n",
    "        # Load pre trained model\n",
    "        self.loaded_model = joblib.load(\"logistic_regression.pkl\")\n",
    "        # Load pre trained model\n",
    "        self.loaded_scaler = joblib.load(\"standard_scaler.pkl\")\n",
    "        # API key to OpenAI chatbot is stored in separate txt file.\n",
    "        with open(\"API_key.txt\", \"r\") as f:\n",
    "            openai.api_key = f.readline().rstrip()\n",
    "        # Average score for fake news is stored in separate txt file.\n",
    "        with open(\"LIWC_average_fake.txt\", \"r\") as f:\n",
    "            self.averages = {\n",
    "                line.rstrip().split(\":\")[0]: line.rstrip().split(\":\")[1]\n",
    "                for line in f.readlines()\n",
    "            }\n",
    "\n",
    "\n",
    "# Process given text with LIWC software, will only work while software is working\n",
    "def LIWC_process(sentence):\n",
    "    LIWC_analysis_result = None\n",
    "\n",
    "    cmd_to_execute = [\n",
    "        \"LIWC-22-cli\",\n",
    "        \"--mode\",\n",
    "        \"wc\",\n",
    "        \"--input\",\n",
    "        \"console\",\n",
    "        \"--console-text\",\n",
    "        sentence,\n",
    "        \"--output\",\n",
    "        \"console\",\n",
    "    ]\n",
    "\n",
    "    # CLI access to the software\n",
    "    try:\n",
    "        results = (\n",
    "            subprocess.check_output(cmd_to_execute, stderr=subprocess.DEVNULL)\n",
    "            .decode()\n",
    "            .strip()\n",
    "            .splitlines()\n",
    "        )\n",
    "        # Only capture the result\n",
    "        LIWC_analysis_result = results[7]\n",
    "    except Exception as e:\n",
    "        print(f\"Error LIWC processing: {sentence}. Error: {e}\")\n",
    "\n",
    "    # Process data into the format we want, we keep the data as Pandas dataframe\n",
    "    LIWC_analysis_result = json.loads(LIWC_analysis_result)\n",
    "    columns = [key for key, _ in LIWC_analysis_result.items()]\n",
    "    values = [value for _, value in LIWC_analysis_result.items()]\n",
    "    pd_dataframe = pd.DataFrame(\n",
    "        {column: [value] for column, value in zip(columns, values)}\n",
    "    )\n",
    "\n",
    "    return pd_dataframe\n",
    "\n",
    "\n",
    "# LIWC analysis will be scaled with pre-trained standard scaler\n",
    "def scale_result(pd_dataframe, loaded_scaler):\n",
    "    columns_to_scale = None\n",
    "\n",
    "    # Load the list of sorted columns used for training\n",
    "    with open(\"columns_to_scale\", \"r\") as f:\n",
    "        columns_to_scale = [line.rstrip() for line in f.readlines()]\n",
    "\n",
    "    pd_dataframe[columns_to_scale] = loaded_scaler.transform(\n",
    "        pd_dataframe[columns_to_scale]\n",
    "    )\n",
    "\n",
    "    return pd_dataframe\n",
    "\n",
    "\n",
    "# Model tries to detect probability if the given sentence is fake news\n",
    "def model_prediction(pd_dataframe, loaded_model):\n",
    "    columns_to_predict = []\n",
    "\n",
    "    # Load the list of sorted columns used for training\n",
    "    with open(\"columns_to_predict\", \"r\") as f:\n",
    "        columns_to_predict = [line.rstrip() for line in f.readlines()]\n",
    "\n",
    "    probability = loaded_model.predict_proba(pd_dataframe[columns_to_predict])\n",
    "    prediction = loaded_model.predict(pd_dataframe[columns_to_predict])\n",
    "\n",
    "    return (prediction[0], probability[0])\n",
    "\n",
    "\n",
    "# Generate short query to be used with Google fact check tools API\n",
    "def query_generation(sentence):\n",
    "    prompt = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"Pretend you are a query builder who can extract keyword from the given article and suggest appropriate very short query.\n",
    "        This query will be given to Google fact check tools API to retrieve related source information.\n",
    "        Please format your response as below so that can easily be used.\n",
    "\n",
    "        Format:\n",
    "        Query: \"Your suggestion here\"\n",
    "        Explanation: \"Your explanation here\"\n",
    "        \"\"\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"Sentence: {sentence}\"\"\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"For example, when the input is 'When Obama was sworn into office, he DID NOT use the Holy Bible, but instead the Kuran (Their equivalency to our Bible, but very different beliefs).', this can be summarised into 'Barack Obama bible kuran'\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Use API cahtbot to generate query\n",
    "    answer = ask_GPT(prompt)\n",
    "    # Extract query from the generated response\n",
    "    if answer != \"\":\n",
    "        query = extract_query_from_content(answer)\n",
    "        return query\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "# Helper function to extract query from the generated response\n",
    "def extract_query_from_content(content):\n",
    "    parts = content.split(\"\\n\")\n",
    "\n",
    "    for part in parts:\n",
    "        if part.startswith(\"Query:\"):\n",
    "            return part.replace(\"Query:\", \"\").strip()\n",
    "\n",
    "\n",
    "# Check with Google fact check tool API if we can find any related sources\n",
    "def google_fact_check_tool(query):\n",
    "    # Define the API endpoint and parameters\n",
    "    endpoint = \"https://factchecktools.googleapis.com/v1alpha1/claims:search\"\n",
    "    results_list = []\n",
    "\n",
    "    params = {\n",
    "        \"query\": query,\n",
    "        \"languageCode\": \"en-US\",\n",
    "        \"pageSize\": 10,\n",
    "        \"key\": \"AIzaSyCiPY5hrNpKHCZ1d-htnrhvQ_EjOFbBi0E\",\n",
    "    }\n",
    "\n",
    "    response = requests.get(endpoint, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        results_list.append(data)\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data: {response.status_code}\")\n",
    "\n",
    "    return results_list\n",
    "\n",
    "\n",
    "# Generate explanation by summing up all related information\n",
    "def explaination_generation(sentence, prediction, probability, resource, analysis):\n",
    "    prompt = None\n",
    "    # Check if we could find any reliable source, and result if any\n",
    "    rating = extract_textual_rating(resource)\n",
    "\n",
    "    # We found the reliable source claiming that this is fake news\n",
    "    if rating == \"False\" or \"Distorts the Facts\":\n",
    "        return False\n",
    "    # We found the reliable source claiming that this is true news\n",
    "    elif rating == \"True\" or \"Half True\" or \"Exagerated\":\n",
    "        return True\n",
    "    # We could not find the reliable source so that tries to provide best guess about the given sentence using our model\n",
    "    else:\n",
    "        return False if prediction == 0 else True\n",
    "\n",
    "\n",
    "# Use OpenAI API to generate response\n",
    "def ask_GPT(prompt):\n",
    "    response = None\n",
    "\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=prompt)\n",
    "        result = response.choices[0].message.content\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"OpenAI API error occured, error: {e}\")\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "\n",
    "# LIWC analysis comparing current sentence with statistics from training dataset\n",
    "def LIWC_analysis(pd_dataframe, system):\n",
    "    analysis_result = \"\"\n",
    "\n",
    "    # For each feature from LIWC analysis using software\n",
    "    for column in pd_dataframe:\n",
    "        # Check what is the average value for this feature\n",
    "        average = system.averages.get(column)\n",
    "        value = pd_dataframe[column].iloc[0]\n",
    "        average = float(average)\n",
    "        value = float(value)\n",
    "        abs_diff = abs(average - value)\n",
    "\n",
    "        # Skip if they are extreme values\n",
    "        if abs_diff == 0 or value == 0:\n",
    "            continue\n",
    "\n",
    "        # Analytic score check\n",
    "        if column == \"Analytic\":\n",
    "            if value > 95:\n",
    "                analysis_result += \"The sentence has extremely high analytic score, fake news might try to mimic legitimate analytic articles\\n\"\n",
    "            if value < (average / 2):\n",
    "                analysis_result += \"The sentence has low analytic score, fake news might try to appeal to emotion\\n\"\n",
    "        # Authentic score check\n",
    "        elif column == \"Authentic\":\n",
    "            if value < 50:\n",
    "                analysis_result += \"The sentence has low authentic score, fake news might hide their true intentions\\n\"\n",
    "        # Tone score check\n",
    "        elif column == \"Tone\":\n",
    "            if value < 50:\n",
    "                analysis_result += \"The sentence has low tone score, fake news might have more negative or anxious tone\\n\"\n",
    "        # Features related to level of reasoning\n",
    "        elif column in [\n",
    "            \"insight\",\n",
    "            \"cause\",\n",
    "            \"cogproc\",\n",
    "            \"discrep\",\n",
    "            \"cognition\",\n",
    "            \"tentat\",\n",
    "            \"certitude\",\n",
    "        ]:\n",
    "            if average > value:\n",
    "                analysis_result += \"The sentence has fewer cognitive words than average, fake news might use fewer cognitive words as they might not provide logical or rational arguments.\\n\"\n",
    "        # Features related to emotional language\n",
    "        elif column in [\n",
    "            \"Affect\",\n",
    "            \"emo_neg\",\n",
    "            \"emo_sad\",\n",
    "            \"emo_anx\",\n",
    "            \"emo_anger\",\n",
    "            \"emotion\",\n",
    "            \"tone_neg\",\n",
    "        ]:\n",
    "            if average < value:\n",
    "                analysis_result += \"The sentence has more emotional words than average, fake news stories often use emotionally charged language to provoke reactions.\\n\"\n",
    "        # Features related to use of personal pronouns\n",
    "        elif column in [\"i\", \"we\", \"you\", \"shehe\", \"they\"]:\n",
    "            if average > value:\n",
    "                analysis_result += \"The sentence has more personal pronouns than average, an overuse of personal pronouns might indicate a subjective or biased perspective.\\n\"\n",
    "        # Features related to time orientation\n",
    "        elif column in [\"focuspresent\"]:\n",
    "            if average > value:\n",
    "                analysis_result += \"The sentence is more present focused than average, fake news might be more present focused, emphasising immediate events or emotions rather than providing historical context.\\n\"\n",
    "        # Features related to perception\n",
    "        elif column in [\"see\", \"hear\", \"feel\", \"Perception\", \"visual\"]:\n",
    "            if average > value:\n",
    "                analysis_result += \"The sentence has more sensory languages than average, the use of words related to seeing or hearing might be indicative of claims without evidence.\\n\"\n",
    "        # Features related to body and health\n",
    "        elif column in [\"physical\", \"health\", \"illness\", \"mental\"]:\n",
    "            if average > value:\n",
    "                analysis_result += \"The sentence has more body and health language than average, an over-emphasis on health related terms might indicate health related hoaxes or myths.\\n\"\n",
    "        # Features related to motion verbs and narrative\n",
    "        elif column in [\"motion\"]:\n",
    "            if average > value:\n",
    "                analysis_result += \"The sentence has more motion verbs than average, excessive use of motion verbs might indicate a narrative being constructed.\\n\"\n",
    "        # Features related to fear and power language\n",
    "        elif column in [\"achieve\", \"power\", \"Drives\"]:\n",
    "            if average > value:\n",
    "                analysis_result += \"The sentence has more achievement and power language than average, fake news might appeal to readers aspirations or fears related to power and achievements.\\n\"\n",
    "\n",
    "    return analysis_result\n",
    "\n",
    "\n",
    "def sentiment_analysis(pd_dataframe, sentence):\n",
    "    # Caculate sentiment analysis scores with Textblob library, add them to the dataframe\n",
    "    pd_dataframe[\"Polarity_txb\"] = float(TextBlob(sentence).sentiment.polarity)\n",
    "    pd_dataframe[\"Subjectivity_txb\"] = float(TextBlob(sentence).sentiment.subjectivity)\n",
    "\n",
    "    # Caculate sentiment analysis scores with Vader library, add them to the dataframe\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    pd_dataframe[\"vader_pos\"] = float(analyzer.polarity_scores(sentence)[\"pos\"])\n",
    "    pd_dataframe[\"vader_neu\"] = float(analyzer.polarity_scores(sentence)[\"neu\"])\n",
    "    pd_dataframe[\"vader_neg\"] = float(analyzer.polarity_scores(sentence)[\"neg\"])\n",
    "    pd_dataframe[\"vader_comp\"] = float(analyzer.polarity_scores(sentence)[\"compound\"])\n",
    "\n",
    "    # Tokenise/clean statements into tokens, count the common words each in fake and true dataset\n",
    "    nltk.download(\"punkt\")\n",
    "    nltk.download(\"stopwords\")\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    # Common words will only be extracted from training dataset to prevent target exposure\n",
    "    pd_dataframe[\"clean_text\"] = sentence.lower().replace(r\"[^\\w\\s]+\", \" \")\n",
    "    pd_dataframe[\"tokens\"] = pd_dataframe[\"clean_text\"].apply(word_tokenize)\n",
    "    pd_dataframe[\"filtered_tokens\"] = pd_dataframe[\"tokens\"].apply(\n",
    "        lambda tokens: [w for w in tokens if not w in stop_words]\n",
    "    )\n",
    "\n",
    "    only_common_in_fake_with_prob = []\n",
    "    with open(\"common_in_fake.txt\") as f:\n",
    "        only_common_in_fake_with_prob = [\n",
    "            (line.rstrip().split()[0], float(line.rstrip().split()[1]))\n",
    "            for line in f.readlines()\n",
    "        ]\n",
    "\n",
    "    only_common_in_true_with_prob = []\n",
    "    with open(\"common_in_true.txt\") as f:\n",
    "        only_common_in_true_with_prob = [\n",
    "            (line.rstrip().split()[0], float(line.rstrip().split()[1]))\n",
    "            for line in f.readlines()\n",
    "        ]\n",
    "\n",
    "    # Add common words score/existance to the dataframe\n",
    "    for column in only_common_in_fake_with_prob:\n",
    "        pd_dataframe[column[0]] = [\n",
    "            column[1] if column[0] in text else 0\n",
    "            for text in pd_dataframe[\"filtered_tokens\"]\n",
    "        ]\n",
    "    pd_dataframe[\"common_words_score_fake\"] = [\n",
    "        sum(\n",
    "            [\n",
    "                tuple[1]\n",
    "                for tuple in only_common_in_fake_with_prob\n",
    "                if tuple[0] in pd_dataframe[\"filtered_tokens\"]\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    for column in only_common_in_true_with_prob:\n",
    "        pd_dataframe[column[0]] = [\n",
    "            column[1] if column[0] in text else 0\n",
    "            for text in pd_dataframe[\"filtered_tokens\"]\n",
    "        ]\n",
    "    pd_dataframe[\"common_words_score_true\"] = [\n",
    "        sum([tuple[1] for tuple in only_common_in_true_with_prob if tuple[0] in text])\n",
    "        for text in pd_dataframe[\"filtered_tokens\"]\n",
    "    ]\n",
    "\n",
    "    pd_dataframe[\"flesch_reading_ease\"] = textstat.flesch_reading_ease(sentence)\n",
    "    pd_dataframe[\"flesch_kincaid_grade\"] = textstat.flesch_kincaid_grade(sentence)\n",
    "    pd_dataframe[\"gunning_fog\"] = textstat.gunning_fog(sentence)\n",
    "    pd_dataframe[\"smog_index\"] = textstat.smog_index(sentence)\n",
    "    pd_dataframe[\"ari\"] = textstat.automated_readability_index(sentence)\n",
    "    pd_dataframe[\"coleman_liau_index\"] = textstat.coleman_liau_index(sentence)\n",
    "    pd_dataframe[\"linsear_write\"] = textstat.linsear_write_formula(sentence)\n",
    "    pd_dataframe[\"dale_chall\"] = textstat.dale_chall_readability_score(sentence)\n",
    "\n",
    "    return pd_dataframe\n",
    "\n",
    "\n",
    "def detect_test(input):\n",
    "    # System initialisation\n",
    "    system = Backend_system()\n",
    "    # Log all errors/stdouts while processing\n",
    "    sys.stderr = open(\"error_log.txt\", \"w\")\n",
    "    # Process article with LIWC software\n",
    "    pd_dataframe = LIWC_process(input)\n",
    "    # Statistical analysis of LIWC result\n",
    "    analysis = LIWC_analysis(pd_dataframe, system)\n",
    "    # Sentiment analysis\n",
    "    pd_dataframe = sentiment_analysis(pd_dataframe, input)\n",
    "    # Scale LIWC result\n",
    "    pd_dataframe = scale_result(pd_dataframe, system.loaded_scaler)\n",
    "    # Predict fake news with pre-trained model\n",
    "    prediction, probability = model_prediction(pd_dataframe, system.loaded_model)\n",
    "    # Generate query with OpenAI API\n",
    "    query = query_generation(input)\n",
    "    # Search related source with Google API\n",
    "    if query:\n",
    "        resource = google_fact_check_tool(query)\n",
    "    else:\n",
    "        resource = [{}]\n",
    "    # Generate user friendly explanation\n",
    "    explanation = explaination_generation(\n",
    "        input, prediction, probability, resource, analysis\n",
    "    )\n",
    "\n",
    "    # Close redirection\n",
    "    sys.stderr.close()\n",
    "    sys.stderr = sys.__stderr__\n",
    "\n",
    "    return explanation\n",
    "\n",
    "\n",
    "# To extract rating from related source\n",
    "def extract_textual_rating(data):\n",
    "    textual_ratings = []\n",
    "\n",
    "    for item in data:\n",
    "        claims = item.get(\"claims\", [])\n",
    "\n",
    "        for claim in claims:\n",
    "            claim_reviews = claim.get(\"claimReview\", [])\n",
    "\n",
    "            for review in claim_reviews:\n",
    "                rating = review.get(\"textualRating\", None)\n",
    "                if rating:\n",
    "                    textual_ratings.append(rating)\n",
    "\n",
    "    return textual_ratings[0] if len(textual_ratings) != 0 else None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data retrieved from data_pipeline\n",
    "test_dataframe = pd.read_csv(\"balanced_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for test\n",
    "execution_times = []\n",
    "TP = 0\n",
    "TN = 0\n",
    "FP = 0\n",
    "FN = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API error occured, error: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)\n",
      "OpenAI API error occured, error: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)\n",
      "OpenAI API error occured, error: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)\n",
      "OpenAI API error occured, error: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)\n",
      "OpenAI API error occured, error: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)\n",
      "OpenAI API error occured, error: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)\n",
      "OpenAI API error occured, error: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)\n",
      "OpenAI API error occured, error: The server is overloaded or not ready yet.\n",
      "OpenAI API error occured, error: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)\n",
      "OpenAI API error occured, error: The server is overloaded or not ready yet.\n",
      "OpenAI API error occured, error: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)\n",
      "OpenAI API error occured, error: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)\n",
      "OpenAI API error occured, error: The server is overloaded or not ready yet.\n",
      "OpenAI API error occured, error: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)\n",
      "OpenAI API error occured, error: The server is overloaded or not ready yet.\n",
      "OpenAI API error occured, error: The server is overloaded or not ready yet.\n",
      "OpenAI API error occured, error: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)\n",
      "OpenAI API error occured, error: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)\n",
      "OpenAI API error occured, error: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)\n",
      "OpenAI API error occured, error: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)\n"
     ]
    }
   ],
   "source": [
    "# To check our system against LIAR test dataset (500 statements)\n",
    "length = len(test_dataframe[\"statement\"])\n",
    "\n",
    "for index, (sentence, label) in enumerate(zip(test_dataframe[\"statement\"], test_dataframe[\"label\"])):\n",
    "    with open(\"record.txt\", \"a\") as f:\n",
    "        f.write(str(index))\n",
    "        f.write(\"\\n\")\n",
    "    start = time.time()\n",
    "    result = detect_test(sentence)\n",
    "    end = time.time()\n",
    "\n",
    "    if result is True and label == 1:\n",
    "        TP += 1\n",
    "    elif result is False and label == 0:\n",
    "        TN += 1\n",
    "    elif result is True and label == 0:\n",
    "        FP += 1\n",
    "    elif result is False and label == 1:\n",
    "        FN += 1\n",
    "    else:\n",
    "        print(result, label)\n",
    "    \n",
    "    execution_times.append(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average execution time: 33.4\n"
     ]
    }
   ],
   "source": [
    "# Average time including timeouts\n",
    "print(f\"average execution time: {round(sum(execution_times) / len(execution_times), 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.467020967326214\n"
     ]
    }
   ],
   "source": [
    "# Code to exclude timeouts\n",
    "total = 0\n",
    "count = 0\n",
    "for time in execution_times:\n",
    "    if time < 100:\n",
    "        total += time\n",
    "        count += 1\n",
    "total /= count\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation for precision, recall, f1, accuracy\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "f1 = (2 * precision * recall) / (precision + recall)\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7440476190476191\n",
      "f1: 0.7485380116959063\n"
     ]
    }
   ],
   "source": [
    "print(f\"accuracy: {accuracy}\")\n",
    "print(f\"f1: {f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
